I1026 17:35:37.588100 12504 caffe.cpp:218] Using GPUs 0
I1026 17:35:37.594509 12504 caffe.cpp:223] GPU 0: GeForce GTX 1080 Ti
I1026 17:35:38.249812 12504 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 100
base_lr: 1e-05
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 4000
snapshot: 100
snapshot_prefix: "/home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train"
solver_mode: GPU
device_id: 0
net: "./train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I1026 17:35:38.250090 12504 solver.cpp:87] Creating training net from net file: ./train_val.prototxt
I1026 17:35:38.250529 12504 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1026 17:35:38.250747 12504 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "./train.txt"
    batch_size: 256
    shuffle: true
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "my-fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "my-fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "my-fc8"
  bottom: "label"
  top: "loss"
}
I1026 17:35:38.250888 12504 layer_factory.hpp:77] Creating layer data
I1026 17:35:38.250905 12504 net.cpp:84] Creating Layer data
I1026 17:35:38.250912 12504 net.cpp:380] data -> data
I1026 17:35:38.250936 12504 net.cpp:380] data -> label
I1026 17:35:38.250950 12504 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: ./train.txt
I1026 17:35:38.251298 12504 hdf5_data_layer.cpp:94] Number of HDF5 files: 1
I1026 17:35:38.253031 12504 hdf5.cpp:32] Datatype class: H5T_FLOAT
I1026 17:35:39.346034 12504 net.cpp:122] Setting up data
I1026 17:35:39.346091 12504 net.cpp:129] Top shape: 256 3 227 227 (39574272)
I1026 17:35:39.346101 12504 net.cpp:129] Top shape: 256 (256)
I1026 17:35:39.346107 12504 net.cpp:137] Memory required for data: 158298112
I1026 17:35:39.346119 12504 layer_factory.hpp:77] Creating layer conv1
I1026 17:35:39.346153 12504 net.cpp:84] Creating Layer conv1
I1026 17:35:39.346184 12504 net.cpp:406] conv1 <- data
I1026 17:35:39.346218 12504 net.cpp:380] conv1 -> conv1
I1026 17:35:39.839126 12504 net.cpp:122] Setting up conv1
I1026 17:35:39.839192 12504 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I1026 17:35:39.839200 12504 net.cpp:137] Memory required for data: 455667712
I1026 17:35:39.839247 12504 layer_factory.hpp:77] Creating layer relu1
I1026 17:35:39.839292 12504 net.cpp:84] Creating Layer relu1
I1026 17:35:39.839309 12504 net.cpp:406] relu1 <- conv1
I1026 17:35:39.839332 12504 net.cpp:367] relu1 -> conv1 (in-place)
I1026 17:35:39.839701 12504 net.cpp:122] Setting up relu1
I1026 17:35:39.839732 12504 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I1026 17:35:39.839748 12504 net.cpp:137] Memory required for data: 753037312
I1026 17:35:39.839761 12504 layer_factory.hpp:77] Creating layer norm1
I1026 17:35:39.839783 12504 net.cpp:84] Creating Layer norm1
I1026 17:35:39.839799 12504 net.cpp:406] norm1 <- conv1
I1026 17:35:39.839817 12504 net.cpp:380] norm1 -> norm1
I1026 17:35:39.840117 12504 net.cpp:122] Setting up norm1
I1026 17:35:39.840127 12504 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I1026 17:35:39.840132 12504 net.cpp:137] Memory required for data: 1050406912
I1026 17:35:39.840137 12504 layer_factory.hpp:77] Creating layer pool1
I1026 17:35:39.840162 12504 net.cpp:84] Creating Layer pool1
I1026 17:35:39.840175 12504 net.cpp:406] pool1 <- norm1
I1026 17:35:39.840191 12504 net.cpp:380] pool1 -> pool1
I1026 17:35:39.840260 12504 net.cpp:122] Setting up pool1
I1026 17:35:39.840276 12504 net.cpp:129] Top shape: 256 96 27 27 (17915904)
I1026 17:35:39.840289 12504 net.cpp:137] Memory required for data: 1122070528
I1026 17:35:39.840333 12504 layer_factory.hpp:77] Creating layer conv2
I1026 17:35:39.840360 12504 net.cpp:84] Creating Layer conv2
I1026 17:35:39.840374 12504 net.cpp:406] conv2 <- pool1
I1026 17:35:39.840395 12504 net.cpp:380] conv2 -> conv2
I1026 17:35:39.848564 12504 net.cpp:122] Setting up conv2
I1026 17:35:39.848616 12504 net.cpp:129] Top shape: 256 256 27 27 (47775744)
I1026 17:35:39.848623 12504 net.cpp:137] Memory required for data: 1313173504
I1026 17:35:39.848652 12504 layer_factory.hpp:77] Creating layer relu2
I1026 17:35:39.848678 12504 net.cpp:84] Creating Layer relu2
I1026 17:35:39.848685 12504 net.cpp:406] relu2 <- conv2
I1026 17:35:39.848696 12504 net.cpp:367] relu2 -> conv2 (in-place)
I1026 17:35:39.848942 12504 net.cpp:122] Setting up relu2
I1026 17:35:39.848963 12504 net.cpp:129] Top shape: 256 256 27 27 (47775744)
I1026 17:35:39.848978 12504 net.cpp:137] Memory required for data: 1504276480
I1026 17:35:39.848994 12504 layer_factory.hpp:77] Creating layer norm2
I1026 17:35:39.849016 12504 net.cpp:84] Creating Layer norm2
I1026 17:35:39.849031 12504 net.cpp:406] norm2 <- conv2
I1026 17:35:39.849048 12504 net.cpp:380] norm2 -> norm2
I1026 17:35:39.849328 12504 net.cpp:122] Setting up norm2
I1026 17:35:39.849349 12504 net.cpp:129] Top shape: 256 256 27 27 (47775744)
I1026 17:35:39.849364 12504 net.cpp:137] Memory required for data: 1695379456
I1026 17:35:39.849378 12504 layer_factory.hpp:77] Creating layer pool2
I1026 17:35:39.849398 12504 net.cpp:84] Creating Layer pool2
I1026 17:35:39.849413 12504 net.cpp:406] pool2 <- norm2
I1026 17:35:39.849432 12504 net.cpp:380] pool2 -> pool2
I1026 17:35:39.849488 12504 net.cpp:122] Setting up pool2
I1026 17:35:39.849509 12504 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I1026 17:35:39.849521 12504 net.cpp:137] Memory required for data: 1739681792
I1026 17:35:39.849535 12504 layer_factory.hpp:77] Creating layer conv3
I1026 17:35:39.849560 12504 net.cpp:84] Creating Layer conv3
I1026 17:35:39.849575 12504 net.cpp:406] conv3 <- pool2
I1026 17:35:39.849592 12504 net.cpp:380] conv3 -> conv3
I1026 17:35:39.870182 12504 net.cpp:122] Setting up conv3
I1026 17:35:39.870239 12504 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I1026 17:35:39.870245 12504 net.cpp:137] Memory required for data: 1806135296
I1026 17:35:39.870270 12504 layer_factory.hpp:77] Creating layer relu3
I1026 17:35:39.870285 12504 net.cpp:84] Creating Layer relu3
I1026 17:35:39.870292 12504 net.cpp:406] relu3 <- conv3
I1026 17:35:39.870304 12504 net.cpp:367] relu3 -> conv3 (in-place)
I1026 17:35:39.870999 12504 net.cpp:122] Setting up relu3
I1026 17:35:39.871014 12504 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I1026 17:35:39.871019 12504 net.cpp:137] Memory required for data: 1872588800
I1026 17:35:39.871024 12504 layer_factory.hpp:77] Creating layer conv4
I1026 17:35:39.871043 12504 net.cpp:84] Creating Layer conv4
I1026 17:35:39.871048 12504 net.cpp:406] conv4 <- conv3
I1026 17:35:39.871057 12504 net.cpp:380] conv4 -> conv4
I1026 17:35:39.891934 12504 net.cpp:122] Setting up conv4
I1026 17:35:39.891971 12504 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I1026 17:35:39.891978 12504 net.cpp:137] Memory required for data: 1939042304
I1026 17:35:39.891993 12504 layer_factory.hpp:77] Creating layer relu4
I1026 17:35:39.892007 12504 net.cpp:84] Creating Layer relu4
I1026 17:35:39.892014 12504 net.cpp:406] relu4 <- conv4
I1026 17:35:39.892024 12504 net.cpp:367] relu4 -> conv4 (in-place)
I1026 17:35:39.893749 12504 net.cpp:122] Setting up relu4
I1026 17:35:39.893764 12504 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I1026 17:35:39.893769 12504 net.cpp:137] Memory required for data: 2005495808
I1026 17:35:39.893774 12504 layer_factory.hpp:77] Creating layer conv5
I1026 17:35:39.893790 12504 net.cpp:84] Creating Layer conv5
I1026 17:35:39.893795 12504 net.cpp:406] conv5 <- conv4
I1026 17:35:39.893805 12504 net.cpp:380] conv5 -> conv5
I1026 17:35:39.907806 12504 net.cpp:122] Setting up conv5
I1026 17:35:39.907847 12504 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I1026 17:35:39.907853 12504 net.cpp:137] Memory required for data: 2049798144
I1026 17:35:39.907914 12504 layer_factory.hpp:77] Creating layer relu5
I1026 17:35:39.907944 12504 net.cpp:84] Creating Layer relu5
I1026 17:35:39.907963 12504 net.cpp:406] relu5 <- conv5
I1026 17:35:39.907989 12504 net.cpp:367] relu5 -> conv5 (in-place)
I1026 17:35:39.908272 12504 net.cpp:122] Setting up relu5
I1026 17:35:39.908293 12504 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I1026 17:35:39.908306 12504 net.cpp:137] Memory required for data: 2094100480
I1026 17:35:39.908320 12504 layer_factory.hpp:77] Creating layer pool5
I1026 17:35:39.908339 12504 net.cpp:84] Creating Layer pool5
I1026 17:35:39.908354 12504 net.cpp:406] pool5 <- conv5
I1026 17:35:39.908375 12504 net.cpp:380] pool5 -> pool5
I1026 17:35:39.908447 12504 net.cpp:122] Setting up pool5
I1026 17:35:39.908468 12504 net.cpp:129] Top shape: 256 256 6 6 (2359296)
I1026 17:35:39.908483 12504 net.cpp:137] Memory required for data: 2103537664
I1026 17:35:39.908496 12504 layer_factory.hpp:77] Creating layer fc6
I1026 17:35:39.908524 12504 net.cpp:84] Creating Layer fc6
I1026 17:35:39.908538 12504 net.cpp:406] fc6 <- pool5
I1026 17:35:39.908555 12504 net.cpp:380] fc6 -> fc6
I1026 17:35:40.425734 12504 net.cpp:122] Setting up fc6
I1026 17:35:40.425776 12504 net.cpp:129] Top shape: 256 4096 (1048576)
I1026 17:35:40.425782 12504 net.cpp:137] Memory required for data: 2107731968
I1026 17:35:40.425801 12504 layer_factory.hpp:77] Creating layer relu6
I1026 17:35:40.425814 12504 net.cpp:84] Creating Layer relu6
I1026 17:35:40.425824 12504 net.cpp:406] relu6 <- fc6
I1026 17:35:40.425837 12504 net.cpp:367] relu6 -> fc6 (in-place)
I1026 17:35:40.426158 12504 net.cpp:122] Setting up relu6
I1026 17:35:40.426180 12504 net.cpp:129] Top shape: 256 4096 (1048576)
I1026 17:35:40.426194 12504 net.cpp:137] Memory required for data: 2111926272
I1026 17:35:40.426208 12504 layer_factory.hpp:77] Creating layer drop6
I1026 17:35:40.426226 12504 net.cpp:84] Creating Layer drop6
I1026 17:35:40.426240 12504 net.cpp:406] drop6 <- fc6
I1026 17:35:40.426259 12504 net.cpp:367] drop6 -> fc6 (in-place)
I1026 17:35:40.426296 12504 net.cpp:122] Setting up drop6
I1026 17:35:40.426312 12504 net.cpp:129] Top shape: 256 4096 (1048576)
I1026 17:35:40.426324 12504 net.cpp:137] Memory required for data: 2116120576
I1026 17:35:40.426337 12504 layer_factory.hpp:77] Creating layer fc7
I1026 17:35:40.426360 12504 net.cpp:84] Creating Layer fc7
I1026 17:35:40.426374 12504 net.cpp:406] fc7 <- fc6
I1026 17:35:40.426390 12504 net.cpp:380] fc7 -> fc7
I1026 17:35:40.659487 12504 net.cpp:122] Setting up fc7
I1026 17:35:40.659546 12504 net.cpp:129] Top shape: 256 4096 (1048576)
I1026 17:35:40.659552 12504 net.cpp:137] Memory required for data: 2120314880
I1026 17:35:40.659569 12504 layer_factory.hpp:77] Creating layer relu7
I1026 17:35:40.659586 12504 net.cpp:84] Creating Layer relu7
I1026 17:35:40.659593 12504 net.cpp:406] relu7 <- fc7
I1026 17:35:40.659605 12504 net.cpp:367] relu7 -> fc7 (in-place)
I1026 17:35:40.659934 12504 net.cpp:122] Setting up relu7
I1026 17:35:40.659943 12504 net.cpp:129] Top shape: 256 4096 (1048576)
I1026 17:35:40.659948 12504 net.cpp:137] Memory required for data: 2124509184
I1026 17:35:40.659952 12504 layer_factory.hpp:77] Creating layer drop7
I1026 17:35:40.659966 12504 net.cpp:84] Creating Layer drop7
I1026 17:35:40.659973 12504 net.cpp:406] drop7 <- fc7
I1026 17:35:40.659981 12504 net.cpp:367] drop7 -> fc7 (in-place)
I1026 17:35:40.660004 12504 net.cpp:122] Setting up drop7
I1026 17:35:40.660012 12504 net.cpp:129] Top shape: 256 4096 (1048576)
I1026 17:35:40.660017 12504 net.cpp:137] Memory required for data: 2128703488
I1026 17:35:40.660022 12504 layer_factory.hpp:77] Creating layer my-fc8
I1026 17:35:40.660032 12504 net.cpp:84] Creating Layer my-fc8
I1026 17:35:40.660038 12504 net.cpp:406] my-fc8 <- fc7
I1026 17:35:40.660048 12504 net.cpp:380] my-fc8 -> my-fc8
I1026 17:35:40.660220 12504 net.cpp:122] Setting up my-fc8
I1026 17:35:40.660228 12504 net.cpp:129] Top shape: 256 1 (256)
I1026 17:35:40.660233 12504 net.cpp:137] Memory required for data: 2128704512
I1026 17:35:40.660259 12504 layer_factory.hpp:77] Creating layer loss
I1026 17:35:40.660267 12504 net.cpp:84] Creating Layer loss
I1026 17:35:40.660274 12504 net.cpp:406] loss <- my-fc8
I1026 17:35:40.660279 12504 net.cpp:406] loss <- label
I1026 17:35:40.660293 12504 net.cpp:380] loss -> loss
I1026 17:35:40.660336 12504 net.cpp:122] Setting up loss
I1026 17:35:40.660342 12504 net.cpp:129] Top shape: (1)
I1026 17:35:40.660346 12504 net.cpp:132]     with loss weight 1
I1026 17:35:40.660382 12504 net.cpp:137] Memory required for data: 2128704516
I1026 17:35:40.660387 12504 net.cpp:198] loss needs backward computation.
I1026 17:35:40.660394 12504 net.cpp:198] my-fc8 needs backward computation.
I1026 17:35:40.660399 12504 net.cpp:198] drop7 needs backward computation.
I1026 17:35:40.660404 12504 net.cpp:198] relu7 needs backward computation.
I1026 17:35:40.660408 12504 net.cpp:198] fc7 needs backward computation.
I1026 17:35:40.660413 12504 net.cpp:198] drop6 needs backward computation.
I1026 17:35:40.660418 12504 net.cpp:198] relu6 needs backward computation.
I1026 17:35:40.660421 12504 net.cpp:198] fc6 needs backward computation.
I1026 17:35:40.660426 12504 net.cpp:198] pool5 needs backward computation.
I1026 17:35:40.660431 12504 net.cpp:198] relu5 needs backward computation.
I1026 17:35:40.660436 12504 net.cpp:198] conv5 needs backward computation.
I1026 17:35:40.660441 12504 net.cpp:198] relu4 needs backward computation.
I1026 17:35:40.660445 12504 net.cpp:198] conv4 needs backward computation.
I1026 17:35:40.660450 12504 net.cpp:198] relu3 needs backward computation.
I1026 17:35:40.660454 12504 net.cpp:198] conv3 needs backward computation.
I1026 17:35:40.660459 12504 net.cpp:198] pool2 needs backward computation.
I1026 17:35:40.660465 12504 net.cpp:198] norm2 needs backward computation.
I1026 17:35:40.660470 12504 net.cpp:198] relu2 needs backward computation.
I1026 17:35:40.660473 12504 net.cpp:198] conv2 needs backward computation.
I1026 17:35:40.660480 12504 net.cpp:198] pool1 needs backward computation.
I1026 17:35:40.660483 12504 net.cpp:198] norm1 needs backward computation.
I1026 17:35:40.660488 12504 net.cpp:198] relu1 needs backward computation.
I1026 17:35:40.660492 12504 net.cpp:198] conv1 needs backward computation.
I1026 17:35:40.660498 12504 net.cpp:200] data does not need backward computation.
I1026 17:35:40.660504 12504 net.cpp:242] This network produces output loss
I1026 17:35:40.660523 12504 net.cpp:255] Network initialization done.
I1026 17:35:40.660863 12504 solver.cpp:172] Creating test net (#0) specified by net file: ./train_val.prototxt
I1026 17:35:40.660899 12504 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1026 17:35:40.661103 12504 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "./test.txt"
    batch_size: 128
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "my-fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "my-fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "my-fc8"
  bottom: "label"
  top: "loss"
}
I1026 17:35:40.661190 12504 layer_factory.hpp:77] Creating layer data
I1026 17:35:40.661200 12504 net.cpp:84] Creating Layer data
I1026 17:35:40.661204 12504 net.cpp:380] data -> data
I1026 17:35:40.661213 12504 net.cpp:380] data -> label
I1026 17:35:40.661221 12504 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: ./test.txt
I1026 17:35:40.661553 12504 hdf5_data_layer.cpp:94] Number of HDF5 files: 1
I1026 17:35:41.184741 12504 net.cpp:122] Setting up data
I1026 17:35:41.184800 12504 net.cpp:129] Top shape: 128 3 227 227 (19787136)
I1026 17:35:41.184810 12504 net.cpp:129] Top shape: 128 (128)
I1026 17:35:41.184859 12504 net.cpp:137] Memory required for data: 79149056
I1026 17:35:41.184871 12504 layer_factory.hpp:77] Creating layer conv1
I1026 17:35:41.184904 12504 net.cpp:84] Creating Layer conv1
I1026 17:35:41.184940 12504 net.cpp:406] conv1 <- data
I1026 17:35:41.184955 12504 net.cpp:380] conv1 -> conv1
I1026 17:35:41.190289 12504 net.cpp:122] Setting up conv1
I1026 17:35:41.190312 12504 net.cpp:129] Top shape: 128 96 55 55 (37171200)
I1026 17:35:41.190320 12504 net.cpp:137] Memory required for data: 227833856
I1026 17:35:41.190343 12504 layer_factory.hpp:77] Creating layer relu1
I1026 17:35:41.190358 12504 net.cpp:84] Creating Layer relu1
I1026 17:35:41.190366 12504 net.cpp:406] relu1 <- conv1
I1026 17:35:41.190376 12504 net.cpp:367] relu1 -> conv1 (in-place)
I1026 17:35:41.192356 12504 net.cpp:122] Setting up relu1
I1026 17:35:41.192368 12504 net.cpp:129] Top shape: 128 96 55 55 (37171200)
I1026 17:35:41.192373 12504 net.cpp:137] Memory required for data: 376518656
I1026 17:35:41.192379 12504 layer_factory.hpp:77] Creating layer norm1
I1026 17:35:41.192391 12504 net.cpp:84] Creating Layer norm1
I1026 17:35:41.192396 12504 net.cpp:406] norm1 <- conv1
I1026 17:35:41.192404 12504 net.cpp:380] norm1 -> norm1
I1026 17:35:41.194653 12504 net.cpp:122] Setting up norm1
I1026 17:35:41.194669 12504 net.cpp:129] Top shape: 128 96 55 55 (37171200)
I1026 17:35:41.194674 12504 net.cpp:137] Memory required for data: 525203456
I1026 17:35:41.194680 12504 layer_factory.hpp:77] Creating layer pool1
I1026 17:35:41.194692 12504 net.cpp:84] Creating Layer pool1
I1026 17:35:41.194699 12504 net.cpp:406] pool1 <- norm1
I1026 17:35:41.194706 12504 net.cpp:380] pool1 -> pool1
I1026 17:35:41.194758 12504 net.cpp:122] Setting up pool1
I1026 17:35:41.194768 12504 net.cpp:129] Top shape: 128 96 27 27 (8957952)
I1026 17:35:41.194773 12504 net.cpp:137] Memory required for data: 561035264
I1026 17:35:41.194778 12504 layer_factory.hpp:77] Creating layer conv2
I1026 17:35:41.194795 12504 net.cpp:84] Creating Layer conv2
I1026 17:35:41.194800 12504 net.cpp:406] conv2 <- pool1
I1026 17:35:41.194808 12504 net.cpp:380] conv2 -> conv2
I1026 17:35:41.212970 12504 net.cpp:122] Setting up conv2
I1026 17:35:41.213012 12504 net.cpp:129] Top shape: 128 256 27 27 (23887872)
I1026 17:35:41.213017 12504 net.cpp:137] Memory required for data: 656586752
I1026 17:35:41.213039 12504 layer_factory.hpp:77] Creating layer relu2
I1026 17:35:41.213085 12504 net.cpp:84] Creating Layer relu2
I1026 17:35:41.213101 12504 net.cpp:406] relu2 <- conv2
I1026 17:35:41.213122 12504 net.cpp:367] relu2 -> conv2 (in-place)
I1026 17:35:41.215512 12504 net.cpp:122] Setting up relu2
I1026 17:35:41.215523 12504 net.cpp:129] Top shape: 128 256 27 27 (23887872)
I1026 17:35:41.215528 12504 net.cpp:137] Memory required for data: 752138240
I1026 17:35:41.215531 12504 layer_factory.hpp:77] Creating layer norm2
I1026 17:35:41.215541 12504 net.cpp:84] Creating Layer norm2
I1026 17:35:41.215546 12504 net.cpp:406] norm2 <- conv2
I1026 17:35:41.215555 12504 net.cpp:380] norm2 -> norm2
I1026 17:35:41.217885 12504 net.cpp:122] Setting up norm2
I1026 17:35:41.217896 12504 net.cpp:129] Top shape: 128 256 27 27 (23887872)
I1026 17:35:41.217901 12504 net.cpp:137] Memory required for data: 847689728
I1026 17:35:41.217906 12504 layer_factory.hpp:77] Creating layer pool2
I1026 17:35:41.217921 12504 net.cpp:84] Creating Layer pool2
I1026 17:35:41.217926 12504 net.cpp:406] pool2 <- norm2
I1026 17:35:41.217932 12504 net.cpp:380] pool2 -> pool2
I1026 17:35:41.217980 12504 net.cpp:122] Setting up pool2
I1026 17:35:41.217998 12504 net.cpp:129] Top shape: 128 256 13 13 (5537792)
I1026 17:35:41.218013 12504 net.cpp:137] Memory required for data: 869840896
I1026 17:35:41.218026 12504 layer_factory.hpp:77] Creating layer conv3
I1026 17:35:41.218050 12504 net.cpp:84] Creating Layer conv3
I1026 17:35:41.218065 12504 net.cpp:406] conv3 <- pool2
I1026 17:35:41.218083 12504 net.cpp:380] conv3 -> conv3
I1026 17:35:41.234179 12504 net.cpp:122] Setting up conv3
I1026 17:35:41.234213 12504 net.cpp:129] Top shape: 128 384 13 13 (8306688)
I1026 17:35:41.234251 12504 net.cpp:137] Memory required for data: 903067648
I1026 17:35:41.234273 12504 layer_factory.hpp:77] Creating layer relu3
I1026 17:35:41.234302 12504 net.cpp:84] Creating Layer relu3
I1026 17:35:41.234318 12504 net.cpp:406] relu3 <- conv3
I1026 17:35:41.234339 12504 net.cpp:367] relu3 -> conv3 (in-place)
I1026 17:35:41.234594 12504 net.cpp:122] Setting up relu3
I1026 17:35:41.234612 12504 net.cpp:129] Top shape: 128 384 13 13 (8306688)
I1026 17:35:41.234627 12504 net.cpp:137] Memory required for data: 936294400
I1026 17:35:41.234640 12504 layer_factory.hpp:77] Creating layer conv4
I1026 17:35:41.234664 12504 net.cpp:84] Creating Layer conv4
I1026 17:35:41.234678 12504 net.cpp:406] conv4 <- conv3
I1026 17:35:41.234699 12504 net.cpp:380] conv4 -> conv4
I1026 17:35:41.248610 12504 net.cpp:122] Setting up conv4
I1026 17:35:41.248658 12504 net.cpp:129] Top shape: 128 384 13 13 (8306688)
I1026 17:35:41.248662 12504 net.cpp:137] Memory required for data: 969521152
I1026 17:35:41.248682 12504 layer_factory.hpp:77] Creating layer relu4
I1026 17:35:41.248703 12504 net.cpp:84] Creating Layer relu4
I1026 17:35:41.248709 12504 net.cpp:406] relu4 <- conv4
I1026 17:35:41.248723 12504 net.cpp:367] relu4 -> conv4 (in-place)
I1026 17:35:41.248996 12504 net.cpp:122] Setting up relu4
I1026 17:35:41.249018 12504 net.cpp:129] Top shape: 128 384 13 13 (8306688)
I1026 17:35:41.249032 12504 net.cpp:137] Memory required for data: 1002747904
I1026 17:35:41.249047 12504 layer_factory.hpp:77] Creating layer conv5
I1026 17:35:41.249078 12504 net.cpp:84] Creating Layer conv5
I1026 17:35:41.249094 12504 net.cpp:406] conv5 <- conv4
I1026 17:35:41.249114 12504 net.cpp:380] conv5 -> conv5
I1026 17:35:41.268386 12504 net.cpp:122] Setting up conv5
I1026 17:35:41.268437 12504 net.cpp:129] Top shape: 128 256 13 13 (5537792)
I1026 17:35:41.268442 12504 net.cpp:137] Memory required for data: 1024899072
I1026 17:35:41.268470 12504 layer_factory.hpp:77] Creating layer relu5
I1026 17:35:41.268486 12504 net.cpp:84] Creating Layer relu5
I1026 17:35:41.268492 12504 net.cpp:406] relu5 <- conv5
I1026 17:35:41.268503 12504 net.cpp:367] relu5 -> conv5 (in-place)
I1026 17:35:41.268839 12504 net.cpp:122] Setting up relu5
I1026 17:35:41.268864 12504 net.cpp:129] Top shape: 128 256 13 13 (5537792)
I1026 17:35:41.268880 12504 net.cpp:137] Memory required for data: 1047050240
I1026 17:35:41.268895 12504 layer_factory.hpp:77] Creating layer pool5
I1026 17:35:41.268913 12504 net.cpp:84] Creating Layer pool5
I1026 17:35:41.268927 12504 net.cpp:406] pool5 <- conv5
I1026 17:35:41.268944 12504 net.cpp:380] pool5 -> pool5
I1026 17:35:41.269032 12504 net.cpp:122] Setting up pool5
I1026 17:35:41.269048 12504 net.cpp:129] Top shape: 128 256 6 6 (1179648)
I1026 17:35:41.269062 12504 net.cpp:137] Memory required for data: 1051768832
I1026 17:35:41.269075 12504 layer_factory.hpp:77] Creating layer fc6
I1026 17:35:41.269101 12504 net.cpp:84] Creating Layer fc6
I1026 17:35:41.269115 12504 net.cpp:406] fc6 <- pool5
I1026 17:35:41.269134 12504 net.cpp:380] fc6 -> fc6
I1026 17:35:41.791517 12504 net.cpp:122] Setting up fc6
I1026 17:35:41.791573 12504 net.cpp:129] Top shape: 128 4096 (524288)
I1026 17:35:41.791579 12504 net.cpp:137] Memory required for data: 1053865984
I1026 17:35:41.791596 12504 layer_factory.hpp:77] Creating layer relu6
I1026 17:35:41.791615 12504 net.cpp:84] Creating Layer relu6
I1026 17:35:41.791643 12504 net.cpp:406] relu6 <- fc6
I1026 17:35:41.791664 12504 net.cpp:367] relu6 -> fc6 (in-place)
I1026 17:35:41.792582 12504 net.cpp:122] Setting up relu6
I1026 17:35:41.792596 12504 net.cpp:129] Top shape: 128 4096 (524288)
I1026 17:35:41.792601 12504 net.cpp:137] Memory required for data: 1055963136
I1026 17:35:41.792606 12504 layer_factory.hpp:77] Creating layer drop6
I1026 17:35:41.792616 12504 net.cpp:84] Creating Layer drop6
I1026 17:35:41.792623 12504 net.cpp:406] drop6 <- fc6
I1026 17:35:41.792632 12504 net.cpp:367] drop6 -> fc6 (in-place)
I1026 17:35:41.792690 12504 net.cpp:122] Setting up drop6
I1026 17:35:41.792721 12504 net.cpp:129] Top shape: 128 4096 (524288)
I1026 17:35:41.792733 12504 net.cpp:137] Memory required for data: 1058060288
I1026 17:35:41.792747 12504 layer_factory.hpp:77] Creating layer fc7
I1026 17:35:41.792764 12504 net.cpp:84] Creating Layer fc7
I1026 17:35:41.792778 12504 net.cpp:406] fc7 <- fc6
I1026 17:35:41.792794 12504 net.cpp:380] fc7 -> fc7
I1026 17:35:42.024538 12504 net.cpp:122] Setting up fc7
I1026 17:35:42.024597 12504 net.cpp:129] Top shape: 128 4096 (524288)
I1026 17:35:42.024603 12504 net.cpp:137] Memory required for data: 1060157440
I1026 17:35:42.024621 12504 layer_factory.hpp:77] Creating layer relu7
I1026 17:35:42.024639 12504 net.cpp:84] Creating Layer relu7
I1026 17:35:42.024663 12504 net.cpp:406] relu7 <- fc7
I1026 17:35:42.024686 12504 net.cpp:367] relu7 -> fc7 (in-place)
I1026 17:35:42.025049 12504 net.cpp:122] Setting up relu7
I1026 17:35:42.025070 12504 net.cpp:129] Top shape: 128 4096 (524288)
I1026 17:35:42.025085 12504 net.cpp:137] Memory required for data: 1062254592
I1026 17:35:42.025101 12504 layer_factory.hpp:77] Creating layer drop7
I1026 17:35:42.025122 12504 net.cpp:84] Creating Layer drop7
I1026 17:35:42.025137 12504 net.cpp:406] drop7 <- fc7
I1026 17:35:42.025156 12504 net.cpp:367] drop7 -> fc7 (in-place)
I1026 17:35:42.025192 12504 net.cpp:122] Setting up drop7
I1026 17:35:42.025210 12504 net.cpp:129] Top shape: 128 4096 (524288)
I1026 17:35:42.025224 12504 net.cpp:137] Memory required for data: 1064351744
I1026 17:35:42.025238 12504 layer_factory.hpp:77] Creating layer my-fc8
I1026 17:35:42.025257 12504 net.cpp:84] Creating Layer my-fc8
I1026 17:35:42.025271 12504 net.cpp:406] my-fc8 <- fc7
I1026 17:35:42.025290 12504 net.cpp:380] my-fc8 -> my-fc8
I1026 17:35:42.025487 12504 net.cpp:122] Setting up my-fc8
I1026 17:35:42.025506 12504 net.cpp:129] Top shape: 128 1 (128)
I1026 17:35:42.025518 12504 net.cpp:137] Memory required for data: 1064352256
I1026 17:35:42.025537 12504 layer_factory.hpp:77] Creating layer loss
I1026 17:35:42.025555 12504 net.cpp:84] Creating Layer loss
I1026 17:35:42.025568 12504 net.cpp:406] loss <- my-fc8
I1026 17:35:42.025583 12504 net.cpp:406] loss <- label
I1026 17:35:42.025599 12504 net.cpp:380] loss -> loss
I1026 17:35:42.025652 12504 net.cpp:122] Setting up loss
I1026 17:35:42.025668 12504 net.cpp:129] Top shape: (1)
I1026 17:35:42.025681 12504 net.cpp:132]     with loss weight 1
I1026 17:35:42.025707 12504 net.cpp:137] Memory required for data: 1064352260
I1026 17:35:42.025722 12504 net.cpp:198] loss needs backward computation.
I1026 17:35:42.025738 12504 net.cpp:198] my-fc8 needs backward computation.
I1026 17:35:42.025753 12504 net.cpp:198] drop7 needs backward computation.
I1026 17:35:42.025768 12504 net.cpp:198] relu7 needs backward computation.
I1026 17:35:42.025784 12504 net.cpp:198] fc7 needs backward computation.
I1026 17:35:42.025799 12504 net.cpp:198] drop6 needs backward computation.
I1026 17:35:42.025813 12504 net.cpp:198] relu6 needs backward computation.
I1026 17:35:42.025828 12504 net.cpp:198] fc6 needs backward computation.
I1026 17:35:42.025846 12504 net.cpp:198] pool5 needs backward computation.
I1026 17:35:42.025861 12504 net.cpp:198] relu5 needs backward computation.
I1026 17:35:42.025876 12504 net.cpp:198] conv5 needs backward computation.
I1026 17:35:42.025890 12504 net.cpp:198] relu4 needs backward computation.
I1026 17:35:42.025905 12504 net.cpp:198] conv4 needs backward computation.
I1026 17:35:42.025920 12504 net.cpp:198] relu3 needs backward computation.
I1026 17:35:42.025935 12504 net.cpp:198] conv3 needs backward computation.
I1026 17:35:42.025949 12504 net.cpp:198] pool2 needs backward computation.
I1026 17:35:42.025964 12504 net.cpp:198] norm2 needs backward computation.
I1026 17:35:42.025980 12504 net.cpp:198] relu2 needs backward computation.
I1026 17:35:42.025995 12504 net.cpp:198] conv2 needs backward computation.
I1026 17:35:42.026010 12504 net.cpp:198] pool1 needs backward computation.
I1026 17:35:42.026026 12504 net.cpp:198] norm1 needs backward computation.
I1026 17:35:42.026054 12504 net.cpp:198] relu1 needs backward computation.
I1026 17:35:42.026082 12504 net.cpp:198] conv1 needs backward computation.
I1026 17:35:42.026098 12504 net.cpp:200] data does not need backward computation.
I1026 17:35:42.026113 12504 net.cpp:242] This network produces output loss
I1026 17:35:42.026144 12504 net.cpp:255] Network initialization done.
I1026 17:35:42.026259 12504 solver.cpp:56] Solver scaffolding done.
I1026 17:35:42.027030 12504 caffe.cpp:155] Finetuning from ../../../pretrained-models/bvlc_alexnet.caffemodel
I1026 17:35:42.315026 12504 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ../../../pretrained-models/bvlc_alexnet.caffemodel
I1026 17:35:42.315076 12504 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1026 17:35:42.315084 12504 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1026 17:35:42.315248 12504 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../../../pretrained-models/bvlc_alexnet.caffemodel
I1026 17:35:42.776635 12504 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1026 17:35:42.897377 12504 net.cpp:744] Ignoring source layer fc8
I1026 17:35:43.218791 12504 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ../../../pretrained-models/bvlc_alexnet.caffemodel
I1026 17:35:43.218839 12504 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1026 17:35:43.218845 12504 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1026 17:35:43.218875 12504 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../../../pretrained-models/bvlc_alexnet.caffemodel
I1026 17:35:43.598330 12504 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1026 17:35:43.718834 12504 net.cpp:744] Ignoring source layer fc8
I1026 17:35:43.757155 12504 caffe.cpp:248] Starting Optimization
I1026 17:35:43.757210 12504 solver.cpp:272] Solving AlexNet
I1026 17:35:43.757216 12504 solver.cpp:273] Learning Rate Policy: step
I1026 17:35:43.760596 12504 solver.cpp:330] Iteration 0, Testing net (#0)
I1026 17:35:49.906823 12504 solver.cpp:397]     Test net output #0: loss = 159.211 (* 1 = 159.211 loss)
I1026 17:35:50.190819 12504 solver.cpp:218] Iteration 0 (0 iter/s, 6.43346s/20 iters), loss = 209.996
I1026 17:35:50.190901 12504 solver.cpp:237]     Train net output #0: loss = 209.996 (* 1 = 209.996 loss)
I1026 17:35:50.190932 12504 sgd_solver.cpp:105] Iteration 0, lr = 1e-05
I1026 17:35:56.843983 12504 solver.cpp:218] Iteration 20 (3.00629 iter/s, 6.65271s/20 iters), loss = 11.8945
I1026 17:35:56.844172 12504 solver.cpp:237]     Train net output #0: loss = 11.8945 (* 1 = 11.8945 loss)
I1026 17:35:56.844184 12504 sgd_solver.cpp:105] Iteration 20, lr = 1e-05
I1026 17:36:04.247856 12504 solver.cpp:218] Iteration 40 (2.70201 iter/s, 7.4019s/20 iters), loss = 7.23387
I1026 17:36:04.247925 12504 solver.cpp:237]     Train net output #0: loss = 7.23387 (* 1 = 7.23387 loss)
I1026 17:36:04.247938 12504 sgd_solver.cpp:105] Iteration 40, lr = 1e-05
I1026 17:36:11.595211 12504 solver.cpp:218] Iteration 60 (2.72213 iter/s, 7.3472s/20 iters), loss = 7.35955
I1026 17:36:11.595343 12504 solver.cpp:237]     Train net output #0: loss = 7.35955 (* 1 = 7.35955 loss)
I1026 17:36:11.595356 12504 sgd_solver.cpp:105] Iteration 60, lr = 1e-05
I1026 17:36:18.921340 12504 solver.cpp:218] Iteration 80 (2.73081 iter/s, 7.32384s/20 iters), loss = 5.90464
I1026 17:36:18.921412 12504 solver.cpp:237]     Train net output #0: loss = 5.90464 (* 1 = 5.90464 loss)
I1026 17:36:18.921423 12504 sgd_solver.cpp:105] Iteration 80, lr = 1e-05
I1026 17:36:25.990192 12504 solver.cpp:447] Snapshotting to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_100.caffemodel
I1026 17:36:27.750860 12504 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_100.solverstate
I1026 17:36:28.323076 12504 solver.cpp:330] Iteration 100, Testing net (#0)
I1026 17:36:34.153612 12504 solver.cpp:397]     Test net output #0: loss = 9.83987 (* 1 = 9.83987 loss)
I1026 17:36:34.403163 12504 solver.cpp:218] Iteration 100 (1.29204 iter/s, 15.4794s/20 iters), loss = 5.78112
I1026 17:36:34.403254 12504 solver.cpp:237]     Train net output #0: loss = 5.78112 (* 1 = 5.78112 loss)
I1026 17:36:34.403270 12504 sgd_solver.cpp:105] Iteration 100, lr = 1e-05
I1026 17:36:40.469575 12504 solver.cpp:218] Iteration 120 (3.29694 iter/s, 6.06624s/20 iters), loss = 5.77198
I1026 17:36:40.469748 12504 solver.cpp:237]     Train net output #0: loss = 5.77198 (* 1 = 5.77198 loss)
I1026 17:36:40.469787 12504 sgd_solver.cpp:105] Iteration 120, lr = 1e-05
I1026 17:36:47.874410 12504 solver.cpp:218] Iteration 140 (2.70103 iter/s, 7.40457s/20 iters), loss = 5.623
I1026 17:36:47.874650 12504 solver.cpp:237]     Train net output #0: loss = 5.623 (* 1 = 5.623 loss)
I1026 17:36:47.874670 12504 sgd_solver.cpp:105] Iteration 140, lr = 1e-05
I1026 17:36:55.070076 12504 solver.cpp:218] Iteration 160 (2.77957 iter/s, 7.19535s/20 iters), loss = 5.53985
I1026 17:36:55.070148 12504 solver.cpp:237]     Train net output #0: loss = 5.53985 (* 1 = 5.53985 loss)
I1026 17:36:55.070159 12504 sgd_solver.cpp:105] Iteration 160, lr = 1e-05
I1026 17:37:02.520684 12504 solver.cpp:218] Iteration 180 (2.68441 iter/s, 7.45044s/20 iters), loss = 4.89262
I1026 17:37:02.520762 12504 solver.cpp:237]     Train net output #0: loss = 4.89262 (* 1 = 4.89262 loss)
I1026 17:37:02.520776 12504 sgd_solver.cpp:105] Iteration 180, lr = 1e-05
I1026 17:37:09.320354 12504 solver.cpp:447] Snapshotting to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_200.caffemodel
I1026 17:37:10.796435 12504 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_200.solverstate
I1026 17:37:11.289244 12504 solver.cpp:330] Iteration 200, Testing net (#0)
I1026 17:37:16.660454 12504 solver.cpp:397]     Test net output #0: loss = 8.73757 (* 1 = 8.73757 loss)
I1026 17:37:16.903344 12504 solver.cpp:218] Iteration 200 (1.39059 iter/s, 14.3824s/20 iters), loss = 4.85321
I1026 17:37:16.903429 12504 solver.cpp:237]     Train net output #0: loss = 4.85321 (* 1 = 4.85321 loss)
I1026 17:37:16.903445 12504 sgd_solver.cpp:105] Iteration 200, lr = 1e-05
I1026 17:37:22.103549 12504 solver.cpp:218] Iteration 220 (3.84611 iter/s, 5.20006s/20 iters), loss = 4.09582
I1026 17:37:22.103690 12504 solver.cpp:237]     Train net output #0: loss = 4.09582 (* 1 = 4.09582 loss)
I1026 17:37:22.103701 12504 sgd_solver.cpp:105] Iteration 220, lr = 1e-05
I1026 17:37:28.692001 12504 solver.cpp:218] Iteration 240 (3.03572 iter/s, 6.58823s/20 iters), loss = 4.27928
I1026 17:37:28.692093 12504 solver.cpp:237]     Train net output #0: loss = 4.27928 (* 1 = 4.27928 loss)
I1026 17:37:28.692109 12504 sgd_solver.cpp:105] Iteration 240, lr = 1e-05
I1026 17:37:35.907786 12504 solver.cpp:218] Iteration 260 (2.77177 iter/s, 7.21561s/20 iters), loss = 3.80263
I1026 17:37:35.907883 12504 solver.cpp:237]     Train net output #0: loss = 3.80263 (* 1 = 3.80263 loss)
I1026 17:37:35.907899 12504 sgd_solver.cpp:105] Iteration 260, lr = 1e-05
I1026 17:37:43.237727 12504 solver.cpp:218] Iteration 280 (2.7286 iter/s, 7.32976s/20 iters), loss = 3.62034
I1026 17:37:43.237813 12504 solver.cpp:237]     Train net output #0: loss = 3.62034 (* 1 = 3.62034 loss)
I1026 17:37:43.237831 12504 sgd_solver.cpp:105] Iteration 280, lr = 1e-05
I1026 17:37:50.084012 12504 solver.cpp:447] Snapshotting to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_300.caffemodel
I1026 17:37:51.665000 12504 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_300.solverstate
I1026 17:37:52.157534 12504 solver.cpp:330] Iteration 300, Testing net (#0)
I1026 17:37:58.959873 12504 solver.cpp:397]     Test net output #0: loss = 8.02425 (* 1 = 8.02425 loss)
I1026 17:37:59.204488 12504 solver.cpp:218] Iteration 300 (1.25262 iter/s, 15.9666s/20 iters), loss = 3.50413
I1026 17:37:59.204605 12504 solver.cpp:237]     Train net output #0: loss = 3.50413 (* 1 = 3.50413 loss)
I1026 17:37:59.204620 12504 sgd_solver.cpp:105] Iteration 300, lr = 1e-05
I1026 17:38:04.427268 12504 solver.cpp:218] Iteration 320 (3.82952 iter/s, 5.22259s/20 iters), loss = 4.09633
I1026 17:38:04.427359 12504 solver.cpp:237]     Train net output #0: loss = 4.09633 (* 1 = 4.09633 loss)
I1026 17:38:04.427376 12504 sgd_solver.cpp:105] Iteration 320, lr = 1e-05
I1026 17:38:09.927675 12504 solver.cpp:218] Iteration 340 (3.6362 iter/s, 5.50024s/20 iters), loss = 3.61862
I1026 17:38:09.927762 12504 solver.cpp:237]     Train net output #0: loss = 3.61862 (* 1 = 3.61862 loss)
I1026 17:38:09.927774 12504 sgd_solver.cpp:105] Iteration 340, lr = 1e-05
I1026 17:38:17.295828 12504 solver.cpp:218] Iteration 360 (2.71523 iter/s, 7.36585s/20 iters), loss = 3.04801
I1026 17:38:17.295909 12504 solver.cpp:237]     Train net output #0: loss = 3.04801 (* 1 = 3.04801 loss)
I1026 17:38:17.295922 12504 sgd_solver.cpp:105] Iteration 360, lr = 1e-05
I1026 17:38:24.670869 12504 solver.cpp:218] Iteration 380 (2.71191 iter/s, 7.37487s/20 iters), loss = 2.94823
I1026 17:38:24.673511 12504 solver.cpp:237]     Train net output #0: loss = 2.94823 (* 1 = 2.94823 loss)
I1026 17:38:24.673552 12504 sgd_solver.cpp:105] Iteration 380, lr = 1e-05
I1026 17:38:31.529156 12504 solver.cpp:447] Snapshotting to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_400.caffemodel
I1026 17:38:33.046393 12504 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_400.solverstate
I1026 17:38:33.671250 12504 solver.cpp:330] Iteration 400, Testing net (#0)
I1026 17:38:42.038302 12504 solver.cpp:397]     Test net output #0: loss = 8.90018 (* 1 = 8.90018 loss)
I1026 17:38:42.205889 12504 solver.cpp:218] Iteration 400 (1.14075 iter/s, 17.5323s/20 iters), loss = 3.41679
I1026 17:38:42.205963 12504 solver.cpp:237]     Train net output #0: loss = 3.41679 (* 1 = 3.41679 loss)
I1026 17:38:42.205976 12504 sgd_solver.cpp:105] Iteration 400, lr = 1e-05
I1026 17:38:47.165655 12504 solver.cpp:218] Iteration 420 (4.03254 iter/s, 4.95965s/20 iters), loss = 3.24813
I1026 17:38:47.165784 12504 solver.cpp:237]     Train net output #0: loss = 3.24813 (* 1 = 3.24813 loss)
I1026 17:38:47.165815 12504 sgd_solver.cpp:105] Iteration 420, lr = 1e-05
I1026 17:38:52.376888 12504 solver.cpp:218] Iteration 440 (3.838 iter/s, 5.21104s/20 iters), loss = 2.78867
I1026 17:38:52.376960 12504 solver.cpp:237]     Train net output #0: loss = 2.78867 (* 1 = 2.78867 loss)
I1026 17:38:52.376971 12504 sgd_solver.cpp:105] Iteration 440, lr = 1e-05
I1026 17:38:58.956457 12504 solver.cpp:218] Iteration 460 (3.03978 iter/s, 6.57942s/20 iters), loss = 2.69363
I1026 17:38:58.956632 12504 solver.cpp:237]     Train net output #0: loss = 2.69363 (* 1 = 2.69363 loss)
I1026 17:38:58.956648 12504 sgd_solver.cpp:105] Iteration 460, lr = 1e-05
I1026 17:39:06.176476 12504 solver.cpp:218] Iteration 480 (2.77017 iter/s, 7.21978s/20 iters), loss = 2.43835
I1026 17:39:06.176544 12504 solver.cpp:237]     Train net output #0: loss = 2.43835 (* 1 = 2.43835 loss)
I1026 17:39:06.176556 12504 sgd_solver.cpp:105] Iteration 480, lr = 1e-05
I1026 17:39:12.923759 12504 solver.cpp:447] Snapshotting to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_500.caffemodel
I1026 17:39:14.378943 12504 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_500.solverstate
I1026 17:39:14.867202 12504 solver.cpp:330] Iteration 500, Testing net (#0)
I1026 17:39:24.873348 12504 solver.cpp:397]     Test net output #0: loss = 7.27086 (* 1 = 7.27086 loss)
I1026 17:39:25.039535 12504 solver.cpp:218] Iteration 500 (1.06028 iter/s, 18.8629s/20 iters), loss = 2.3942
I1026 17:39:25.039610 12504 solver.cpp:237]     Train net output #0: loss = 2.3942 (* 1 = 2.3942 loss)
I1026 17:39:25.039625 12504 sgd_solver.cpp:105] Iteration 500, lr = 1e-05
I1026 17:39:29.633589 12504 solver.cpp:218] Iteration 520 (4.35356 iter/s, 4.59394s/20 iters), loss = 2.40198
I1026 17:39:29.633785 12504 solver.cpp:237]     Train net output #0: loss = 2.40198 (* 1 = 2.40198 loss)
I1026 17:39:29.633797 12504 sgd_solver.cpp:105] Iteration 520, lr = 1e-05
I1026 17:39:34.847896 12504 solver.cpp:218] Iteration 540 (3.83579 iter/s, 5.21405s/20 iters), loss = 1.93775
I1026 17:39:34.847967 12504 solver.cpp:237]     Train net output #0: loss = 1.93775 (* 1 = 1.93775 loss)
I1026 17:39:34.847995 12504 sgd_solver.cpp:105] Iteration 540, lr = 1e-05
I1026 17:39:41.055747 12504 solver.cpp:218] Iteration 560 (3.2218 iter/s, 6.20772s/20 iters), loss = 2.5434
I1026 17:39:41.055809 12504 solver.cpp:237]     Train net output #0: loss = 2.5434 (* 1 = 2.5434 loss)
I1026 17:39:41.055840 12504 sgd_solver.cpp:105] Iteration 560, lr = 1e-05
I1026 17:39:48.280884 12504 solver.cpp:218] Iteration 580 (2.76817 iter/s, 7.22499s/20 iters), loss = 2.07784
I1026 17:39:48.280992 12504 solver.cpp:237]     Train net output #0: loss = 2.07784 (* 1 = 2.07784 loss)
I1026 17:39:48.281008 12504 sgd_solver.cpp:105] Iteration 580, lr = 1e-05
I1026 17:39:55.032631 12504 solver.cpp:447] Snapshotting to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_600.caffemodel
I1026 17:39:56.496392 12504 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_600.solverstate
I1026 17:39:57.002929 12504 solver.cpp:330] Iteration 600, Testing net (#0)
I1026 17:40:07.695654 12504 solver.cpp:397]     Test net output #0: loss = 6.53339 (* 1 = 6.53339 loss)
I1026 17:40:07.863909 12504 solver.cpp:218] Iteration 600 (1.0213 iter/s, 19.5828s/20 iters), loss = 2.26209
I1026 17:40:07.863983 12504 solver.cpp:237]     Train net output #0: loss = 2.26209 (* 1 = 2.26209 loss)
I1026 17:40:07.864033 12504 sgd_solver.cpp:105] Iteration 600, lr = 1e-05
I1026 17:40:12.307768 12504 solver.cpp:218] Iteration 620 (4.5007 iter/s, 4.44375s/20 iters), loss = 2.38795
I1026 17:40:12.307860 12504 solver.cpp:237]     Train net output #0: loss = 2.38795 (* 1 = 2.38795 loss)
I1026 17:40:12.307893 12504 sgd_solver.cpp:105] Iteration 620, lr = 1e-05
I1026 17:40:17.523236 12504 solver.cpp:218] Iteration 640 (3.83487 iter/s, 5.21531s/20 iters), loss = 1.81943
I1026 17:40:17.523322 12504 solver.cpp:237]     Train net output #0: loss = 1.81943 (* 1 = 1.81943 loss)
I1026 17:40:17.523339 12504 sgd_solver.cpp:105] Iteration 640, lr = 1e-05
I1026 17:40:23.515004 12504 solver.cpp:218] Iteration 660 (3.338 iter/s, 5.99161s/20 iters), loss = 1.60928
I1026 17:40:23.515094 12504 solver.cpp:237]     Train net output #0: loss = 1.60928 (* 1 = 1.60928 loss)
I1026 17:40:23.515127 12504 sgd_solver.cpp:105] Iteration 660, lr = 1e-05
I1026 17:40:30.740026 12504 solver.cpp:218] Iteration 680 (2.76822 iter/s, 7.22486s/20 iters), loss = 1.69864
I1026 17:40:30.740104 12504 solver.cpp:237]     Train net output #0: loss = 1.69864 (* 1 = 1.69864 loss)
I1026 17:40:30.740118 12504 sgd_solver.cpp:105] Iteration 680, lr = 1e-05
I1026 17:40:37.507165 12504 solver.cpp:447] Snapshotting to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_700.caffemodel
I1026 17:40:39.006405 12504 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_700.solverstate
I1026 17:40:39.520196 12504 solver.cpp:330] Iteration 700, Testing net (#0)
I1026 17:40:50.567840 12504 solver.cpp:397]     Test net output #0: loss = 7.25908 (* 1 = 7.25908 loss)
I1026 17:40:50.736372 12504 solver.cpp:218] Iteration 700 (1.00019 iter/s, 19.9962s/20 iters), loss = 1.62278
I1026 17:40:50.736448 12504 solver.cpp:237]     Train net output #0: loss = 1.62278 (* 1 = 1.62278 loss)
I1026 17:40:50.736461 12504 sgd_solver.cpp:105] Iteration 700, lr = 1e-05
I1026 17:40:55.077673 12504 solver.cpp:218] Iteration 720 (4.60702 iter/s, 4.34121s/20 iters), loss = 1.9837
I1026 17:40:55.077736 12504 solver.cpp:237]     Train net output #0: loss = 1.9837 (* 1 = 1.9837 loss)
I1026 17:40:55.077769 12504 sgd_solver.cpp:105] Iteration 720, lr = 1e-05
I1026 17:41:00.290340 12504 solver.cpp:218] Iteration 740 (3.83691 iter/s, 5.21253s/20 iters), loss = 1.52022
I1026 17:41:00.290416 12504 solver.cpp:237]     Train net output #0: loss = 1.52022 (* 1 = 1.52022 loss)
I1026 17:41:00.290429 12504 sgd_solver.cpp:105] Iteration 740, lr = 1e-05
I1026 17:41:06.190996 12504 solver.cpp:218] Iteration 760 (3.38953 iter/s, 5.90052s/20 iters), loss = 1.5147
I1026 17:41:06.191081 12504 solver.cpp:237]     Train net output #0: loss = 1.5147 (* 1 = 1.5147 loss)
I1026 17:41:06.191093 12504 sgd_solver.cpp:105] Iteration 760, lr = 1e-05
I1026 17:41:13.424137 12504 solver.cpp:218] Iteration 780 (2.76511 iter/s, 7.23298s/20 iters), loss = 1.40625
I1026 17:41:13.424348 12504 solver.cpp:237]     Train net output #0: loss = 1.40625 (* 1 = 1.40625 loss)
I1026 17:41:13.424363 12504 sgd_solver.cpp:105] Iteration 780, lr = 1e-05
I1026 17:41:20.193888 12504 solver.cpp:447] Snapshotting to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_800.caffemodel
I1026 17:41:21.693382 12504 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/hzzone/1tb/bone-age-model/bysex/female/female_alexnet_train_iter_800.solverstate
I1026 17:41:22.210106 12504 solver.cpp:330] Iteration 800, Testing net (#0)
I1026 17:41:33.254267 12504 solver.cpp:397]     Test net output #0: loss = 6.84754 (* 1 = 6.84754 loss)
I1026 17:41:33.587608 12504 solver.cpp:218] Iteration 800 (0.991909 iter/s, 20.1631s/20 iters), loss = 1.31802
I1026 17:41:33.587688 12504 solver.cpp:237]     Train net output #0: loss = 1.31802 (* 1 = 1.31802 loss)
I1026 17:41:33.587703 12504 sgd_solver.cpp:105] Iteration 800, lr = 1e-05
I1026 17:41:37.924676 12504 solver.cpp:218] Iteration 820 (4.61153 iter/s, 4.33696s/20 iters), loss = 1.84155
I1026 17:41:37.924768 12504 solver.cpp:237]     Train net output #0: loss = 1.84155 (* 1 = 1.84155 loss)
I1026 17:41:37.924784 12504 sgd_solver.cpp:105] Iteration 820, lr = 1e-05
I1026 17:41:43.128813 12504 solver.cpp:218] Iteration 840 (3.84321 iter/s, 5.20399s/20 iters), loss = 1.75396
I1026 17:41:43.128890 12504 solver.cpp:237]     Train net output #0: loss = 1.75396 (* 1 = 1.75396 loss)
I1026 17:41:43.128901 12504 sgd_solver.cpp:105] Iteration 840, lr = 1e-05
I1026 17:41:49.014569 12504 solver.cpp:218] Iteration 860 (3.39812 iter/s, 5.88562s/20 iters), loss = 1.30672
I1026 17:41:49.014739 12504 solver.cpp:237]     Train net output #0: loss = 1.30672 (* 1 = 1.30672 loss)
I1026 17:41:49.014770 12504 sgd_solver.cpp:105] Iteration 860, lr = 1e-05
